# БАЗА ПО ТЕРВЕРУ

---

### **1. Пространство элементарных исходов. Случайные события, операции над событиями. Классификация. Алгебра и сигма-алгебра.**

#### **Основные понятия**

*   **Опыт (Эксперимент)**: Любое действие, которое может быть воспроизведено в определенных условиях и приводит к одному из нескольких возможных исходов.
*   **Элементарный исход (ω)**: Каждый из возможных, неразделимых результатов опыта.
*   **Пространство элементарных исходов (Ω)**: Множество *всех* возможных элементарных исходов опыта.
    *   *Пример 1 (Монета)*: Ω = {Орёл, Решка}
    *   *Пример 2 (Игральная кость)*: Ω = {1, 2, 3, 4, 5, 6}

*   **Случайное событие (A, B, C, ...)**: Любое подмножество пространства элементарных исходов (A ⊆ Ω). Событие происходит, если в результате опыта наступает один из элементарных исходов, входящих в это событие.
    *   *Пример (Игральная кость)*: Событие A = "выпало четное число". A = {2, 4, 6}.

#### **Классификация случайных событий**

1.  **Достоверное событие**: Событие, которое *обязательно* произойдет в результате опыта. Оно совпадает со всем пространством Ω.
    *   *Пример*: Выпадение числа от 1 до 6 при броске кости.
2.  **Невозможное событие**: Событие, которое *никогда* не может произойти. Является пустым множеством (∅).
    *   *Пример*: Выпадение числа 7 при броске стандартной кости.
3.  **Случайное событие**: Событие, которое может как произойти, так и не произойти. Это любое подмножество Ω, кроме ∅ и самого Ω.

#### **Операции над событиями (аналогичны операциям над множествами)**

*   **Сумма (Объединение) событий A + B (или A ∪ B)**: Событие, состоящее в том, что произошло *хотя бы одно* из событий A или B.
*   **Произведение (Пересечение) событий A · B (или A ∩ B)**: Событие, состоящее в том, что произошли *оба* события A и B одновременно.
*   **Разность событий A \ B**: Событие, состоящее в том, что A произошло, а B — нет.
*   **Противоположное событие (Дополнение) Ā**: Событие, состоящее в том, что событие A *не произошло*. Ā = Ω \ A.

#### **Совместные и несовместные события**

*   **Совместные события**: Два события, которые могут произойти одновременно в одном опыте. Их пересечение не пусто (A ∩ B ≠ ∅).
    *   *Пример*: A = "выпало четное", B = "выпало число > 3". Их пересечение {4, 6} не пусто.
*   **Несовместные события**: Два события, которые не могут произойти одновременно. Их пересечение пусто (A ∩ B = ∅).
    *   *Пример*: A = "выпало четное", B = "выпало нечетное". Они не могут произойти вместе.

#### **Алгебра и Сигма-алгебра событий**

Для того чтобы корректно определять вероятность, нам нужно рассматривать не произвольные подмножества Ω, а определенную систему подмножеств, обладающую хорошими свойствами.

*   **Алгебра событий (F)**: Непустая система подмножеств F множества Ω, замкнутая относительно операций дополнения и *конечного* числа объединений (и пересечений).
    1.  Ω ∈ F (достоверное событие всегда включается).
    2.  Если A ∈ F, то Ā ∈ F (замкнутость относительно дополнения).
    3.  Если A ∈ F и B ∈ F, то A ∪ B ∈ F (замкнутость относительно конечных объединений).

*   **Сигма-алгебра событий (σ-алгебра, F)**: Алгебра событий, которая замкнута относительно *счетного* числа объединений (и пересечений). Это более сильное требование, необходимое для работы с бесконечными, особенно непрерывными, пространствами исходов.
    1.  Ω ∈ F.
    2.  Если A ∈ F, то Ā ∈ F.
    3.  Если A₁, A₂, ... ∈ F (счетное множество событий), то их объединение ∪Aᵢ также принадлежит F.

**Ключевая идея**: Сигма-алгебра F — это набор "измеримых" событий, для которых мы сможем корректно определить вероятность.

---

### **2. Аксиоматическое определение вероятности. Вероятностное пространство. Свойства. Примеры.**

#### **Вероятностное пространство**

Это математическая модель случайного эксперимента. Представляет собой тройку **(Ω, F, P)**, где:
*   **Ω** — пространство элементарных исходов.
*   **F** — сигма-алгебра событий (набор "хороших" подмножеств Ω).
*   **P** — вероятностная мера (вероятность), т.е. функция P: F → [0, 1], которая каждому событию A из F ставит в соответствие число P(A) и удовлетворяет аксиомам Колмогорова.

#### **Аксиомы вероятности (А. Н. Колмогоров)**

1.  **Аксиома неотрицательности**: Для любого события A ∈ F, вероятность P(A) ≥ 0.
2.  **Аксиома нормировки**: Вероятность достоверного события равна единице: P(Ω) = 1.
3.  **Аксиома счетной аддитивности**: Для любой *счетной* последовательности попарно *несовместных* событий A₁, A₂, ... (т.е. Aᵢ ∩ Aⱼ = ∅ при i ≠ j), вероятность их объединения равна сумме их вероятностей:
    P(∪ Aᵢ) = Σ P(Aᵢ).

#### **Свойства вероятности (следствия из аксиом)**

*   **P(∅) = 0**: Вероятность невозможного события равна нулю.
*   **P(A) ≤ 1**: Вероятность любого события не превосходит 1.
*   **P(Ā) = 1 - P(A)**: Вероятность противоположного события.
*   **P(A ∪ B) = P(A) + P(B) - P(A ∩ B)**: Формула сложения вероятностей для *совместных* событий.
*   Если A ⊆ B, то P(A) ≤ P(B).

#### **Примеры вероятностных пространств**

1.  **Классическое определение вероятности**:
    *   **Условия**: Пространство исходов Ω конечно, и все исходы **равновозможны**.
    *   **Формула**: P(A) = m / n, где:
        *   n — общее число элементарных исходов.
        *   m — число исходов, благоприятствующих событию A.
    *   *Пример*: Вероятность выпадения четного числа на кости. n=6 (исходы {1,2,3,4,5,6}), m=3 (исходы {2,4,6}). P(A) = 3/6 = 1/2.

2.  **Статистическое (частотное) определение вероятности**:
    *   **Идея**: Вероятность оценивается через относительную частоту появления события в длинной серии опытов.
    *   **Формула**: P(A) ≈ W(A) = k / N, где:
        *   N — общее число проведенных опытов.
        *   k — число опытов, в которых событие A произошло.
    *   **Закон больших чисел**: При N → ∞, относительная частота W(A) сходится по вероятности к истинной вероятности P(A).

3.  **Геометрическое определение вероятности**:
    *   **Условия**: Пространство исходов Ω — это некое геометрическое множество (отрезок, область на плоскости, тело в пространстве), имеющее конечную меру (длину, площадь, объем). Все точки этого множества **равновозможны**.
    *   **Формула**: P(A) = μ(A) / μ(Ω), где:
        *   μ(A) — мера (длина, площадь, объем) подмножества, соответствующего событию A.
        *   μ(Ω) — мера всего пространства исходов.
    *   *Пример*: На отрезок [0, 10] наугад бросается точка. Какова вероятность, что она попадет в интервал [2, 5]? P(A) = Длина([2, 5]) / Длина([0, 10]) = 3 / 10.

---

### **3. Условная вероятность. Независимость. Формула полной вероятности и формула Байеса.**

#### **Условная вероятность**

*   **Определение**: **Условная вероятность** события A при условии, что событие B уже произошло (и P(B) > 0), обозначается P(A|B) и вычисляется по формуле:
    **P(A|B) = P(A ∩ B) / P(B)**
*   **Интуиция**: Мы сужаем наше пространство исходов до Ω' = B. Вероятность P(A|B) — это доля "площади" пересечения A и B в "новой вселенной" B.

#### **Вероятность произведения событий (правило умножения)**

Из формулы условной вероятности напрямую следует:
**P(A ∩ B) = P(B) · P(A|B) = P(A) · P(B|A)**

#### **Независимость случайных событий**

*   **Определение**: События A и B называются **независимыми**, если наступление одного из них не изменяет вероятность наступления другого.
*   **Математический критерий**: A и B независимы тогда и только тогда, когда:
    **P(A ∩ B) = P(A) · P(B)**
*   Для независимых событий P(A|B) = P(A) и P(B|A) = P(B).
*   **Независимость в совокупности**: События A₁, A₂, ..., Aₙ независимы в совокупности, если для любого подмножества индексов {i₁, ..., iₖ} выполняется P(Aᵢ₁ ∩ ... ∩ Aᵢₖ) = P(Aᵢ₁) · ... · P(Aᵢₖ). *Попарная независимость не гарантирует независимость в совокупности!*

#### **Полная группа событий (Гипотезы)**

*   **Определение**: Набор событий H₁, H₂, ..., Hₙ образует **полную группу**, если:
    1.  Они попарно несовместны (Hᵢ ∩ Hⱼ = ∅ при i ≠ j).
    2.  Их объединение совпадает со всем пространством Ω (H₁ ∪ H₂ ∪ ... ∪ Hₙ = Ω).
*   **Идея**: В результате опыта обязательно произойдет ровно одна из гипотез. Они "разбивают" всё пространство исходов на непересекающиеся части.

#### **Формула полной вероятности**

*   **Назначение**: Позволяет найти вероятность некоторого события A, если известны вероятности гипотез Hᵢ и условные вероятности P(A|Hᵢ).
*   **Формула**: Пусть H₁, H₂, ..., Hₙ — полная группа событий. Тогда для любого события A:
    **P(A) = Σ P(Hᵢ) · P(A|Hᵢ)**
*   **Интуиция**: Мы вычисляем "полную" вероятность A, суммируя её "кусочки", взвешенные по вероятностям гипотез, через которые событие A может реализоваться.

#### **Формула Байеса (Теорема гипотез)**

*   **Назначение**: Позволяет "переоценить" вероятность гипотезы Hₖ *после* того, как стало известно, что событие A произошло. То есть найти P(Hₖ|A).
*   **Формула**:
    **P(Hₖ|A) = (P(Hₖ) · P(A|Hₖ)) / P(A)**
    Знаменатель P(A) обычно вычисляется по формуле полной вероятности:
    **P(Hₖ|A) = (P(Hₖ) · P(A|Hₖ)) / (Σ P(Hᵢ) · P(A|Hᵢ))**
*   **Терминология**:
    *   P(Hₖ) — **априорная** (доопытная) вероятность гипотезы.
    *   P(Hₖ|A) — **апостериорная** (послеопытная) вероятность гипотезы, уточненная с учетом данных об A.

---

### **4. Повторные независимые опыты. Формула Бернулли. Приближения.**

#### **Схема Бернулли**

Рассматривается серия из **n** **независимых** опытов, в каждом из которых может произойти только два исхода:
*   "Успех" (событие A) с вероятностью **p = P(A)**.
*   "Неудача" (событие Ā) с вероятностью **q = 1 - p**.
Вероятность p (и q) постоянна во всех опытах.

#### **Формула Бернулли**

*   **Назначение**: Вычисляет вероятность того, что в **n** опытах схемы Бернулли "успех" произойдет ровно **k** раз.
*   **Формула**: **Pₙ(k) = Cₙᵏ · pᵏ · qⁿ⁻ᵏ**
    где Cₙᵏ = n! / (k! · (n-k)!) — число сочетаний (биномиальный коэффициент).

*   **Вывод формулы**:
    1.  Рассмотрим один конкретный исход, где k успехов и (n-k) неудач (например, УУ...У НН...Н). Так как опыты независимы, вероятность такого исхода равна pᵏ · qⁿ⁻ᵏ.
    2.  Существует много таких исходов. Число способов выбрать k позиций для успехов из n возможных равно числу сочетаний Cₙᵏ.
    3.  Все эти исходы несовместны, поэтому по аксиоме аддитивности мы складываем их вероятности. Итого: Pₙ(k) = Cₙᵏ · pᵏ · qⁿ⁻ᵏ.

#### **Приближения формулы Бернулли (при большом n)**

Прямой расчет по формуле Бернулли при больших n становится трудоемким.

1.  **Формула Пуассона (закон редких событий)**
    *   **Условия применения**: n → ∞, p → 0, но их произведение **λ = np** остается постоянной (небольшой) величиной.
    *   **Формула**: **Pₙ(k) ≈ (λᵏ / k!) · e⁻ˡ**
    *   **Интуиция**: Моделирует число редких событий за определенный интервал времени или пространства.

2.  **Локальная формула Муавра-Лапласа**
    *   **Условия применения**: n → ∞, p не близко к 0 или 1 (обычно проверяют, что npq > 9).
    *   **Назначение**: Приближенно вычисляет вероятность того, что событие произойдет ровно **k** раз.
    *   **Формула**: **Pₙ(k) ≈ (1 / √(npq)) · φ(x)**
        где:
        *   **φ(x) = (1 / √(2π)) · e⁻ˣ²/²** — функция плотности стандартного нормального распределения.
        *   **x = (k - np) / √(npq)** — стандартизованное значение.
        *   np — мат. ожидание, √(npq) — стандартное отклонение биномиального распределения.

3.  **Интегральная формула Муавра-Лапласа**
    *   **Условия применения**: Те же, что и для локальной.
    *   **Назначение**: Приближенно вычисляет вероятность того, что число успехов k будет лежать в интервале [k₁, k₂].
    *   **Формула**: **Pₙ(k₁ ≤ k ≤ k₂) ≈ Φ(x₂) - Φ(x₁)**
        где:
        *   **Φ(x) = ∫₋∞ˣ φ(t) dt** — функция Лапласа, интегральная функция стандартного нормального распределения.
        *   **x₁ = (k₁ - np) / √(npq)**
        *   **x₂ = (k₂ - np) / √(npq)**

---

### **5. Дискретные случайные величины (ДСВ). Законы распределения.**

*   **Случайная величина (СВ)**: Величина, которая в результате опыта принимает одно из возможных числовых значений, причем заранее неизвестно, какое именно. Формально, это функция ξ: Ω → ℝ.
*   **Дискретная СВ**: Случайная величина, множество возможных значений которой конечно или счетно.

#### **Законы распределения ДСВ**

Это любое правило, устанавливающее связь между возможными значениями ДСВ и их вероятностями.

1.  **Ряд распределения**:
    *   Наиболее распространенный способ задания закона распределения ДСВ. Представляет собой таблицу:
        | xᵢ | x₁ | x₂ | ... | xₙ |
        |---|---|---|---|---|
        | pᵢ | p₁ | p₂ | ... | pₙ |
    *   Здесь xᵢ — возможные значения СВ, pᵢ = P(ξ = xᵢ) — их вероятности.
    *   **Свойство**: Сумма всех вероятностей равна 1: Σ pᵢ = 1.

2.  **Функция распределения (CDF - Cumulative Distribution Function)**:
    *   Универсальный способ задания закона распределения, подходящий и для дискретных, и для непрерывных СВ.
    *   **Определение**: **F(x) = P(ξ ≤ x)**. Для любого числа x функция F(x) показывает вероятность того, что СВ примет значение, меньшее или равное x.
    *   **Для ДСВ**: F(x) = Σ_{xᵢ ≤ x} pᵢ.
    *   **Свойства F(x)**:
        *   0 ≤ F(x) ≤ 1.
        *   F(x) — неубывающая функция.
        *   F(-∞) = 0, F(+∞) = 1.
        *   F(x) непрерывна справа.
    *   **График F(x) для ДСВ**: Ступенчатая функция, скачки происходят в точках xᵢ, и высота скачка равна pᵢ.

#### **Примеры распределений ДСВ**

*   **Распределение Бернулли**: Описывает один опыт с двумя исходами (1 - успех, 0 - неудача). P(ξ=1) = p, P(ξ=0) = q.
*   **Биномиальное распределение B(n, p)**: Описывает число успехов в n испытаниях Бернулли. P(ξ=k) = Cₙᵏ pᵏ qⁿ⁻ᵏ.
*   **Распределение Пуассона Π(λ)**: Описывает число редких событий. P(ξ=k) = (λᵏ / k!) e⁻ˡ.
*   **Геометрическое распределение**: Описывает число опытов до первого успеха. P(ξ=k) = qᵏ⁻¹ p.

---

### **6. Непрерывные случайные величины (НСВ). Законы распределения.**

*   **Непрерывная СВ**: Случайная величина, возможные значения которой сплошь заполняют некоторый интервал (конечный или бесконечный).
*   **Ключевая особенность**: Вероятность того, что НСВ примет одно конкретное значение, равна нулю: **P(ξ = c) = 0**. Поэтому для НСВ бессмысленно строить ряд распределения.

#### **Законы распределения НСВ**

1.  **Функция распределения (CDF)**:
    *   **Определение**: То же, что и для ДСВ: **F(x) = P(ξ ≤ x)**.
    *   **Свойства**: Те же, что и для ДСВ, но для абсолютно непрерывных СВ функция F(x) является **непрерывной**.
    *   **Вероятность попадания в интервал**: P(a < ξ < b) = F(b) - F(a).

2.  **Плотность распределения (PDF - Probability Density Function)**:
    *   **Определение**: **f(x) = F'(x)**. Плотность — это производная от функции распределения.
    *   **Интуиция**: f(x) характеризует "плотность" вероятности в окрестности точки x. Чем выше f(x), тем вероятнее СВ примет значение вблизи x.
    *   **Свойства f(x)**:
        *   f(x) ≥ 0 для всех x.
        *   **Условие нормировки**: ∫₋∞⁺∞ f(x) dx = 1 (полная площадь под кривой плотности равна 1).
    *   **Связь с F(x)**: F(x) = ∫₋∞ˣ f(t) dt.
    *   **Вероятность попадания в интервал**: P(a < ξ < b) = ∫ₐᵇ f(x) dx (площадь под кривой плотности на интервале).

#### **Примеры распределений НСВ**

*   **Равномерное распределение U[a, b]**: СВ принимает значения на отрезке [a, b], и все значения одинаково вероятны.
    *   f(x) = 1/(b-a) при x ∈ [a, b], и 0 в остальных случаях.
*   **Показательное (экспоненциальное) распределение Exp(λ)**: Описывает время до наступления некоторого события (например, время безотказной работы прибора).
    *   f(x) = λe⁻ˡˣ при x ≥ 0, и 0 при x < 0.
*   **Нормальное (Гауссово) распределение N(μ, σ²)**: Самое важное распределение в теории вероятностей. Описывает многие природные и социальные явления.
    *   f(x) = (1 / (σ√(2π))) · e⁻⁽ˣ⁻ᵐ⁾²/⁽²ˢ²⁾
    *   μ — математическое ожидание (центр симметрии).
    *   σ — среднеквадратическое отклонение (определяет "ширину" колокола).

---

### **7. Числовые характеристики случайных величин: мат. ожидание, дисперсия, мода, медиана, квантили.**

Числовые характеристики — это одиночные числа, которые описывают ключевые свойства распределения случайной величины (положение, разброс, форму).

#### **Математическое ожидание (M[ξ] или E[ξ])**

*   **Определение**: Среднее взвешенное значение случайной величины. Центр распределения, "центр масс".
*   **Формулы**:
    *   **Для ДСВ**: M[ξ] = Σ xᵢ pᵢ (сумма произведений значений на их вероятности).
    *   **Для НСВ**: M[ξ] = ∫₋∞⁺∞ x · f(x) dx.
*   **Свойства**:
    *   M[C] = C (мат. ожидание константы равно самой константе).
    *   M[Cξ] = C · M[ξ] (константу можно выносить).
    *   M[ξ + η] = M[ξ] + M[η] (мат. ожидание суммы равно сумме мат. ожиданий, **независимость не требуется!**).

#### **Дисперсия (D[ξ] или Var[ξ])**

*   **Определение**: Математическое ожидание квадрата отклонения случайной величины от её математического ожидания. Характеризует **разброс** или **изменчивость** значений СВ вокруг её среднего.
*   **Основная формула**: D[ξ] = M[(ξ - M[ξ])²].
*   **Расчетная формула (более удобная)**: **D[ξ] = M[ξ²] - (M[ξ])²**
    *   (Мат. ожидание квадрата минус квадрат мат. ожидания).
    *   Для вычисления M[ξ²] используются формулы:
        *   Для ДСВ: M[ξ²] = Σ xᵢ² pᵢ
        *   Для НСВ: M[ξ²] = ∫₋∞⁺∞ x² · f(x) dx
*   **Свойства**:
    *   D[C] = 0 (у константы нет разброса).
    *   D[Cξ] = C² · D[ξ] (константа выносится в квадрате!).
    *   D[ξ + C] = D[ξ] (сдвиг на константу не меняет разброс).
    *   Для **независимых** СВ ξ и η: D[ξ + η] = D[ξ] + D[η].

#### **Среднеквадратическое отклонение (СКО, σ[ξ])**

*   **Определение**: Корень квадратный из дисперсии. **σ[ξ] = √D[ξ]**.
*   **Преимущество**: Имеет ту же размерность, что и сама случайная величина, что делает его более интуитивно понятным для оценки разброса.

#### **Характеристики положения (структурные средние)**

*   **Мода (Mo)**: Наиболее вероятное значение случайной величины.
    *   **Для ДСВ**: Значение xᵢ с наибольшей вероятностью pᵢ.
    *   **Для НСВ**: Значение x, в котором плотность распределения f(x) достигает максимума.
    *   Распределение может иметь несколько мод (мультимодальное) или не иметь их вовсе.

*   **Медиана (Me)**: Значение, которое делит распределение пополам.
    *   **Определение**: Такое число `me`, что P(ξ ≤ me) = 0.5 и P(ξ ≥ me) = 0.5.
    *   Находится из уравнения **F(me) = 0.5**, где F(x) — функция распределения.

*   **Квантиль порядка p (xₚ)**: Обобщение медианы.
    *   **Определение**: Значение xₚ, для которого **F(xₚ) = p**, где p ∈ (0, 1).
    *   Медиана — это квантиль порядка 0.5. Квартили — квантили порядка 0.25, 0.5, 0.75.

---

### **8. Функции случайной величины. Закон распределения и числовые характеристики.**

Пусть ξ — случайная величина с известным законом распределения, и η = φ(ξ) — новая случайная величина, являющаяся функцией от ξ. Задача — найти закон распределения и характеристики η.

#### **Закон распределения η = φ(ξ)**

1.  **Если ξ — ДСВ**:
    *   **Алгоритм**:
        1.  Найти все возможные значения yⱼ = φ(xᵢ) для новой СВ η.
        2.  Если разным xᵢ соответствуют одинаковые yⱼ, то вероятности этих xᵢ нужно сложить.
        3.  Составить новый ряд распределения для η.
    *   *Пример*: ξ имеет ряд: `x={-1, 0, 1}`, `p={0.2, 0.5, 0.3}`. Найти распределение η = ξ².
        *   y₁ = (-1)² = 1, y₂ = 0² = 0, y₃ = 1² = 1.
        *   Возможные значения для η: {0, 1}.
        *   P(η=0) = P(ξ=0) = 0.5.
        *   P(η=1) = P(ξ=-1) + P(ξ=1) = 0.2 + 0.3 = 0.5.
        *   Итоговый ряд для η: `y={0, 1}`, `p={0.5, 0.5}`.

2.  **Если ξ — НСВ**:
    *   **Общий метод (через функцию распределения)**:
        1.  Находим функцию распределения η: G(y) = P(η < y) = P(φ(ξ) < y).
        2.  Выражаем неравенство φ(ξ) < y через ξ. Например, если η = ξ³, то ξ < ³√y.
        3.  Вычисляем вероятность этого неравенства через известную функцию распределения F(x) величины ξ.
        4.  Находим плотность распределения g(y) для η, дифференцируя G(y): g(y) = G'(y).
    *   **Частный случай (для монотонной функции φ)**:
        Если φ(x) строго монотонна, то существует обратная функция x = ψ(y). Тогда плотность g(y) можно найти по формуле:
        **g(y) = f(ψ(y)) · |ψ'(y)|**

#### **Числовые характеристики η = φ(ξ)**

Для нахождения числовых характеристик η **не обязательно** находить ее закон распределения!

*   **Математическое ожидание (правило "бессознательного статистика")**:
    *   **Для ДСВ**: M[η] = M[φ(ξ)] = **Σ φ(xᵢ) · pᵢ**.
    *   **Для НСВ**: M[η] = M[φ(ξ)] = **∫₋∞⁺∞ φ(x) · f(x) dx**.
*   **Дисперсия**: Используем расчетную формулу D[η] = M[η²] - (M[η])².
    *   M[η] находится по формулам выше.
    *   M[η²] = M[(φ(ξ))²] находится аналогично:
        *   Для ДСВ: M[η²] = Σ (φ(xᵢ))² · pᵢ.
        *   Для НСВ: M[η²] = ∫₋∞⁺∞ (φ(x))² · f(x) dx.

---

### **9. Функции нормальных случайных величин. Распределения Пирсона, Стьюдента и Фишера.**

Эти распределения являются фундаментальными в математической статистике и получаются как функции от независимых нормально распределенных случайных величин.

Пусть ξ₁, ξ₂, ..., ξₙ — независимые случайные величины, имеющие **стандартное нормальное распределение N(0, 1)**.

1.  **Распределение χ² (хи-квадрат) Пирсона**:
    *   **Определение**: Сумма квадратов **n** независимых стандартных нормальных СВ.
        **χ²ₙ = Σᵢ (ξᵢ)²**
    *   **Параметр**: `n` — **число степеней свободы**. Оно определяет форму распределения.
    *   **Свойства**:
        *   Принимает только неотрицательные значения.
        *   Асимметрично, с "хвостом" вправо.
        *   Сумма двух независимых χ²-величин с n₁ и n₂ степенями свободы есть χ²-величина с (n₁+n₂) степенями свободы.
    *   **Применение**: Проверка гипотез о согласии (goodness-of-fit tests), анализ таблиц сопряженности, построение доверительных интервалов для дисперсии.

2.  **Распределение Стьюдента (t-распределение)**:
    *   **Определение**: Отношение стандартной нормальной СВ к корню из независимой от неё χ²-величины, деленной на число её степеней свободы.
        **tₙ = ξ₀ / √(χ²ₙ / n)**
    *   **Параметр**: `n` — **число степеней свободы** (от χ²).
    *   **Свойства**:
        *   Симметричное, колоколообразное, похожее на нормальное, но с более "тяжелыми" хвостами (больше вероятность экстремальных значений).
        *   При n → ∞, распределение Стьюдента стремится к стандартному нормальному N(0, 1).
    *   **Применение**: Построение доверительных интервалов и проверка гипотез о математическом ожидании при **неизвестной дисперсии** и малом объеме выборки.

3.  **Распределение Фишера (F-распределение)**:
    *   **Определение**: Отношение двух независимых χ²-величин, каждая из которых поделена на свое число степеней свободы.
        **F(k₁, k₂) = (χ²ₖ₁ / k₁) / (χ²ₖ₂ / k₂)**
    *   **Параметры**: `k₁` и `k₂` — **числа степеней свободы** числителя и знаменателя.
    *   **Свойства**:
        *   Принимает только неотрицательные значения.
        *   Асимметрично.
    *   **Применение**: Проверка гипотезы о равенстве дисперсий двух нормальных совокупностей, дисперсионный анализ (ANOVA).

---

### **10. Системы двух дискретных случайных величин. Законы и частные распределения.**

*   **Система двух СВ (случайный вектор)**: Рассматриваем пару (ξ, η), значениями которой являются пары чисел (xᵢ, yⱼ).

#### **Закон распределения системы двух ДСВ**

*   **Таблица распределения (матрица)**: Наиболее наглядный способ.
    |       | y₁   | y₂   | ...  | yₘ   |
    |-------|------|------|------|------|
    | **x₁**  | p₁₁  | p₁₂  | ...  | p₁ₘ  |
    | **x₂**  | p₂₁  | p₂₂  | ...  | p₂ₘ  |
    | **...** | ...  | ...  | ...  | ...  |
    | **xₙ**  | pₙ₁  | pₙ₂  | ...  | pₙₘ  |
*   **pᵢⱼ = P(ξ = xᵢ, η = yⱼ)** — вероятность того, что ξ примет значение xᵢ *и одновременно* η примет значение yⱼ.
*   **Свойство нормировки**: Сумма всех вероятностей в таблице равна 1: **Σᵢ Σⱼ pᵢⱼ = 1**.

#### **Частные (маргинальные) распределения**

Это законы распределения каждой из величин (ξ и η) в отдельности.

*   **Как найти**: Чтобы найти распределение ξ, нужно просуммировать вероятности по столбцам для каждого значения xᵢ. Чтобы найти распределение η, нужно суммировать по строкам для каждого yⱼ.
*   **Формулы**:
    *   **P(ξ = xᵢ) = pᵢ. = Σⱼ pᵢⱼ** (сумма по j-тому столбцу для i-той строки)
    *   **P(η = yⱼ) = p.ⱼ = Σᵢ pᵢⱼ** (сумма по i-той строке для j-того столбца)

#### **Функция распределения системы ДСВ**

*   **Определение**: F(x, y) = P(ξ ≤ x, η ≤ y).
*   **Вычисление**: F(x, y) = Σ_{xᵢ ≤ x} Σ_{yⱼ ≤ y} pᵢⱼ. (Суммируем вероятности для всех пар (xᵢ, yⱼ), где оба компонента меньше или равны (x, y)).

---

### **11. Системы двух непрерывных случайных величин. Законы и частные распределения.**

#### **Законы распределения системы двух НСВ**

1.  **Совместная функция распределения (Joint CDF)**:
    *   **Определение**: F(x, y) = P(ξ ≤ x, η ≤ y).
    *   **Свойства**: Аналогичны одномерному случаю (неубывающая по каждому аргументу, пределы 0 и 1, и т.д.).

2.  **Совместная плотность распределения (Joint PDF)**:
    *   **Определение**: f(x, y) = ∂²F(x, y) / (∂x ∂y).
    *   **Свойства**:
        *   f(x, y) ≥ 0.
        *   **Нормировка**: ∫₋∞⁺∞ ∫₋∞⁺∞ f(x, y) dx dy = 1 (объем под поверхностью плотности равен 1).
    *   **Вероятность попадания в область D**: P((ξ, η) ∈ D) = ∬_D f(x, y) dx dy.

#### **Частные (маргинальные) распределения**

*   **Как найти**: Чтобы найти плотность одной переменной, нужно "проинтегрировать" (исключить) другую переменную.
*   **Формулы**:
    *   **Плотность ξ**: f_ξ(x) = ∫₋∞⁺∞ f(x, y) dy.
    *   **Плотность η**: f_η(y) = ∫₋∞⁺∞ f(x, y) dx.

---

### **12. Условные распределения. Кривые регрессии.**

#### **Условные законы распределения**

*   **Идея**: Закон распределения одной СВ при условии, что другая СВ приняла конкретное значение.
*   **Для ДСВ**: Условная вероятность P(η = yⱼ | ξ = xᵢ) = pᵢⱼ / P(ξ = xᵢ) = pᵢⱼ / pᵢ.
    *   Для каждого фиксированного xᵢ мы получаем новый закон распределения для η.
*   **Для НСВ**: Условная плотность распределения f(y|x) = f(x, y) / f_ξ(x).
    *   Эта функция, рассматриваемая как функция от `y` (при фиксированном `x`), является полноценной плотностью распределения (интеграл по `y` равен 1).

#### **Условное математическое ожидание**

*   **Определение**: Математическое ожидание, вычисленное по условному закону распределения.
*   **M(η | ξ = x)** — это **функция** от `x`. Она показывает, как в среднем изменяется η в зависимости от значения ξ.
*   **Формулы**:
    *   **Для ДСВ**: M(η | ξ = xᵢ) = Σⱼ yⱼ · P(η = yⱼ | ξ = xᵢ).
    *   **Для НСВ**: M(η | ξ = x) = ∫₋∞⁺∞ y · f(y|x) dy.

#### **Кривые регрессии**

*   **Определение**: График условного математического ожидания.
*   **Регрессия η на ξ**: y = M(η | ξ = x). Показывает среднее значение η для заданного x.
*   **Регрессия ξ на η**: x = M(ξ | η = y). Показывает среднее значение ξ для заданного y.
*   **Смысл**: Кривая регрессии является наилучшим прогнозом одной переменной на основе другой в смысле минимизации среднеквадратичной ошибки.

---

### **13. Числовые характеристики системы. Ковариация и коэффициент корреляции.**

#### **Ковариация (Cov(ξ, η))**

*   **Определение**: Математическое ожидание произведения отклонений случайных величин от их математических ожиданий.
    **Cov(ξ, η) = M[(ξ - M[ξ])(η - M[η])]**.
*   **Расчетная формула**: **Cov(ξ, η) = M[ξη] - M[ξ]M[η]**.
    *   M[ξη] = Σᵢ Σⱼ xᵢyⱼ pᵢⱼ (для ДСВ) или ∫∫ xy f(x,y) dxdy (для НСВ).
*   **Свойства**:
    *   Cov(ξ, η) = Cov(η, ξ).
    *   Cov(ξ, ξ) = D[ξ].
    *   Если ξ и η **независимы**, то **Cov(ξ, η) = 0**.
    *   **Внимание!** Обратное неверно: из Cov(ξ, η) = 0 не следует независимость. Это означает лишь отсутствие *линейной* зависимости.

#### **Коэффициент корреляции (ρ)**

*   **Проблема ковариации**: Зависит от единиц измерения СВ.
*   **Определение**: Нормированная ковариация.
    **ρ(ξ, η) = Cov(ξ, η) / (σ[ξ] · σ[η])**
*   **Свойства**:
    *   Безразмерная величина.
    *   **-1 ≤ ρ ≤ 1**.
    *   |ρ| = 1 <=> существует точная **линейная** зависимость η = aξ + b.
    *   ρ = 0 <=> величины **некоррелированы** (нет линейной связи).
    *   Знак ρ показывает направление связи: ρ > 0 (положительная связь - "чем больше ξ, тем в среднем больше η"), ρ < 0 (отрицательная).
    *   |ρ| показывает **силу линейной связи**.

---

### **14. Прямые среднеквадратической регрессии (с выводом).**

*   **Задача**: Кривые регрессии M(η|x) могут быть сложными. Часто их аппроксимируют прямой линией ŷ = a x + b, которая является "наилучшей" в некотором смысле.
*   **Критерий**: Минимизация средней квадратичной ошибки прогноза.
    **S(a, b) = M[(η - ŷ)²] = M[(η - (ax + b))²] → min**

#### **Вывод формул**

1.  **Целевая функция**: S(a, b) = M[η² - 2η(ax + b) + (ax + b)²]
    Раскрываем скобки и используем линейность мат. ожидания:
    S(a, b) = M[η²] - 2aM[ξη] - 2bM[η] + a²M[ξ²] + 2abM[ξ] + b²

2.  **Находим минимум**: Необходимое условие экстремума — равенство нулю частных производных по `a` и `b`.
    *   **∂S/∂b = -2M[η] + 2aM[ξ] + 2b = 0**  =>  b = M[η] - aM[ξ]   (Уравнение 1)
    *   **∂S/∂a = -2M[ξη] + 2aM[ξ²] + 2bM[ξ] = 0** => aM[ξ²] + bM[ξ] = M[ξη]  (Уравнение 2)

3.  **Решаем систему**: Подставляем `b` из (1) в (2):
    aM[ξ²] + (M[η] - aM[ξ])M[ξ] = M[ξη]
    aM[ξ²] + M[η]M[ξ] - a(M[ξ])² = M[ξη]
    a(M[ξ²] - (M[ξ])²) = M[ξη] - M[ξ]M[η]

4.  **Получаем коэффициенты**:
    *   В скобках слева стоит D[ξ], справа — Cov(ξ, η).
        **a = Cov(ξ, η) / D[ξ]**
    *   Подставляем `a` в формулу для `b`:
        **b = M[η] - (Cov(ξ, η) / D[ξ]) · M[ξ]**

5.  **Итоговое уравнение прямой регрессии η на ξ**:
    ŷ = ax + b => ŷ = (Cov/Dξ)x + Mη - (Cov/Dξ)Mξ
    **ŷ - M[η] = (Cov(ξ, η) / D[ξ]) · (x - M[ξ])**

    Используя, что a = ρ·(σ_η/σ_ξ), получаем канонический вид:
    **ŷ - M[η] = ρ · (σ_η / σ_ξ) · (x - M[ξ])**

Аналогично выводится прямая регрессии ξ на η:
**x̂ - M[ξ] = ρ · (σ_ξ / σ_η) · (y - M[η])**

---

### **15. Закон распределения и характеристики суммы случайных величин.**

#### **Закон распределения суммы Z = ξ + η**

*   **Задача**: Найти закон распределения Z, зная законы распределения ξ и η. Решается просто только для **независимых** СВ.
*   **Формула свертки (композиции)**:
    *   **Для ДСВ**: P(Z = z) = Σᵢ P(ξ = xᵢ) · P(η = z - xᵢ).
    *   **Для НСВ**: f_Z(z) = ∫₋∞⁺∞ f_ξ(x) · f_η(z - x) dx. (Интеграл свертки).

*   **Важнейшее свойство нормального распределения**: Сумма независимых нормальных СВ также распределена нормально.
    Если ξ ~ N(μ₁, σ₁²) и η ~ N(μ₂, σ₂²), то Z = ξ + η ~ N(μ₁ + μ₂, σ₁² + σ₂²).

#### **Математическое ожидание суммы**

*   **M[ξ₁ + ξ₂ + ... + ξₙ] = M[ξ₁] + M[ξ₂] + ... + M[ξₙ]**
*   Это свойство выполняется всегда, **независимо от того, зависимы СВ или нет**.

#### **Дисперсия суммы**

*   **Для двух СВ (общий случай)**:
    **D[ξ + η] = D[ξ] + D[η] + 2Cov(ξ, η)**
*   **Для n СВ (общий случай)**:
    D[Σξᵢ] = ΣD[ξᵢ] + 2Σ_{i<j} Cov(ξᵢ, ξⱼ)
*   **Для НЕЗАВИСИМЫХ СВ**: Cov(ξᵢ, ξⱼ) = 0 для i≠j. Формула упрощается:
    **D[ξ₁ + ξ₂ + ... + ξₙ] = D[ξ₁] + D[ξ₂] + ... + D[ξₙ]**
    (Дисперсия суммы независимых СВ равна сумме их дисперсий). Это одно из самых фундаментальных свойств в теории вероятностей.


Отлично, продолжаем конспект, переходя к более продвинутым темам теории случайных процессов и математической статистики.

---

### **16. Случайные функции (процессы). Вероятностные характеристики.**

#### **Основные понятия**

*   **Случайный процесс (СП) X(t)**: Это семейство случайных величин, зависящих от некоторого параметра `t`, который чаще всего интерпретируется как время. Обозначается X(t, ω), где `ω` — элементарный исход из Ω.
    *   При фиксированном `t = t₀`, **X(t₀)** — это **случайная величина**. Её называют **сечением** или **ордиантой** процесса.
    *   При фиксированном исходе `ω = ω₀`, **x(t) = X(t, ω₀)** — это **неслучайная функция времени**, называемая **реализацией** или **траекторией** процесса.

*   **Классификация СП**:
    1.  **По типу времени `t`**:
        *   **С дискретным временем**: `t` принимает дискретные значения (t = 0, 1, 2, ...). Называются **случайными последовательностями**.
        *   **С непрерывным временем**: `t` может принимать любые значения из некоторого интервала.
    2.  **По типу значений (состояний) X(t)**:
        *   **С дискретными состояниями**: Множество значений X(t) конечно или счетно.
        *   **С непрерывными состояниями**: Множество значений X(t) — континуум.

#### **Вероятностные характеристики СП**

Поскольку СП — это бесконечномерный объект, его полное описание (аналог закона распределения для СВ) очень сложно. На практике используют его конечномерные распределения и числовые характеристики.

1.  **Одномерное распределение**: Закон распределения сечения X(t) в заданный момент времени `t`. Описывается функцией распределения `F₁(x, t) = P(X(t) < x)` или плотностью `f₁(x, t)`.

2.  **Двумерное распределение**: Совместный закон распределения сечений X(t₁) и X(t₂) в два момента времени `t₁` и `t₂`. Описывается функцией `F₂(x₁, x₂; t₁, t₂) = P(X(t₁) < x₁, X(t₂) < x₂)`.

3.  **Математическое ожидание (средняя функция)**: `m_x(t) = M[X(t)]`. Это неслучайная функция, описывающая среднее значение процесса в каждый момент времени.

4.  **Дисперсия**: `D_x(t) = D[X(t)] = M[(X(t) - m_x(t))²]`. Характеризует разброс реализаций процесса вокруг среднего в момент `t`.

5.  **Корреляционная функция (автокорреляция)**:
    *   **Определение**: `K_x(t₁, t₂) = M[(X(t₁) - m_x(t₁))(X(t₂) - m_x(t₂))]`.
    *   **Смысл**: Характеризует степень **статистической связи** между значениями процесса в два разных момента времени `t₁` и `t₂`. Это ключевая характеристика, описывающая "память" или внутреннюю структуру процесса.

#### **Стационарные случайные процессы**

Это процессы, вероятностные характеристики которых инвариантны относительно сдвига по времени.
*   **Стационарность в узком смысле**: Все конечномерные функции распределения не меняются при сдвиге всех временных аргументов на одну и ту же величину `τ`.
*   **Стационарность в широком смысле (чаще используется)**:
    1.  Мат. ожидание — константа: `m_x(t) = m = const`.
    2.  Корреляционная функция зависит только от разности времен: `K_x(t₁, t₂) = K_x(t₂ - t₁) = K_x(τ)`, где `τ = t₂ - t₁`.

---

### **17. Марковские процессы. Цепи Маркова с дискретным временем.**

*   **Марковский процесс**: Случайный процесс, для которого будущее состояние зависит только от **настоящего** состояния и не зависит от прошлого (от того, как процесс пришел в это состояние). Это **свойство отсутствия памяти**.
    `P(X(tₙ₊₁) < x | X(tₙ)=xₙ, X(tₙ₋₁)=xₙ₋₁,...) = P(X(tₙ₊₁) < x | X(tₙ)=xₙ)`.

*   **Цепь Маркова (ЦМ)**: Марковский процесс с **дискретным** множеством состояний {S₁, S₂, ..., Sₖ}.

#### **ЦМ с дискретным временем**

*   **Переходные вероятности**: `pᵢⱼ = P(Xₙ₊₁ = Sⱼ | Xₙ = Sᵢ)` — вероятность перехода из состояния `Sᵢ` в состояние `Sⱼ` за **один шаг**.
*   **Матрица переходных вероятностей (P)**: Матрица размера k×k, где элемент `P[i,j] = pᵢⱼ`.
    *   **Свойства**: Все `pᵢⱼ ≥ 0` и сумма элементов в каждой строке равна 1 (из состояния `Sᵢ` обязательно произойдет переход в какое-то другое или то же самое состояние).

*   **Распределение вероятностей состояний через n шагов**:
    *   Пусть `π(0) = (p₁(0), p₂(0), ..., pₖ(0))` — вектор-строка начальных вероятностей состояний.
    *   Вероятности состояний после 1 шага: `π(1) = π(0) · P`.
    *   Вероятности состояний после n шагов: **`π(n) = π(0) · Pⁿ`**.
    *   Матрица `Pⁿ` содержит вероятности перехода `pᵢⱼ(n)` из `Sᵢ` в `Sⱼ` за `n` шагов.

*   **Финальное (стационарное) распределение вероятностей**:
    *   **Определение**: Распределение `π = (π₁, π₂, ..., πₖ)`, которое не меняется со временем. Если система достигла этого распределения, она останется в нем навсегда.
    *   **Условие**: **`π = π · P`**.
    *   **Смысл**: Стационарный вектор `π` является **левым собственным вектором** матрицы P, соответствующим собственному значению λ=1.
    *   **Поиск**: Для нахождения `π` нужно решить систему линейных алгебраических уравнений `π = πP` с дополнительным условием нормировки `Σπᵢ = 1`.
    *   **Существование**: Финальное распределение существует и единственно, если цепь **эргодическая** (т.е. из любого состояния можно попасть в любое другое и она не является периодической).

---

### **18. Марковские процессы. Цепи Маркова с непрерывным временем.**

*   **Отличие**: Переходы между состояниями могут происходить в любой момент времени.
*   **Интенсивность перехода (λᵢⱼ)**:
    *   Вместо вероятностей `pᵢⱼ` вводятся **интенсивности (скорости)** перехода.
    *   `λᵢⱼ · dt` — это вероятность перехода из `Sᵢ` в `Sⱼ` за малый промежуток времени `dt` (при i≠j).
*   **Матрица интенсивностей (генератор) Q**:
    *   `Q[i,j] = λᵢⱼ` для i≠j.
    *   Диагональные элементы `Q[i,i] = - Σ_{j≠i} λᵢⱼ = -λᵢ`. `λᵢ` — полная интенсивность выхода из состояния `Sᵢ`.
    *   **Свойство**: Сумма элементов в каждой строке матрицы Q равна 0.

*   **Распределение вероятностей состояний в момент времени t**:
    *   Пусть `pᵢ(t)` — вероятность нахождения системы в состоянии `Sᵢ` в момент `t`.
    *   Динамика `pᵢ(t)` описывается **системой дифференциальных уравнений Колмогорова**:
        **`dpⱼ(t)/dt = (приток вероятности в j) - (отток вероятности из j)`**
        **`dpⱼ(t)/dt = Σ_{i≠j} pᵢ(t)λᵢⱼ - pⱼ(t)λⱼ`**
    *   В матричной форме: **`dπ(t)/dt = π(t) · Q`**, где `π(t)` — вектор-строка вероятностей.
    *   Решение этой системы с начальным условием `π(0)` дает распределение в любой момент времени `t`. `π(t) = π(0)e^{Qt}`.

*   **Финальное (стационарное) распределение**:
    *   При `t → ∞` система приходит в равновесие, `dpⱼ(t)/dt = 0`.
    *   **Условие**: **`π · Q = 0`**.
    *   **Поиск**: Для нахождения стационарного распределения `π` нужно решить систему линейных алгебраических уравнений `πQ = 0` с условием нормировки `Σπᵢ = 1`.

---

### **19. Сходимость по вероятности. Неравенства. Теоремы Чебышёва и Бернулли.**

*   **Сходимость по вероятности**: Последовательность СВ `{ξₙ}` сходится по вероятности к СВ `ξ`, если для любого `ε > 0`:
    `lim_{n→∞} P(|ξₙ - ξ| > ε) = 0`.
    **Интуиция**: С ростом `n` становится всё менее вероятно, что `ξₙ` будет отличаться от `ξ` на какую-либо, даже самую малую, величину `ε`.

*   **Неравенство Маркова**: Для любой **неотрицательной** СВ `ξ` с конечным мат. ожиданием и для любого `a > 0`:
    **`P(ξ ≥ a) ≤ M[ξ] / a`**
    **Смысл**: Дает верхнюю оценку вероятности "большого" значения СВ, зная только ее среднее.

*   **Неравенство Чебышёва**: Для любой СВ `ξ` с конечными M[ξ] и D[ξ], для любого `ε > 0`:
    **`P(|ξ - M[ξ]| ≥ ε) ≤ D[ξ] / ε²`**
    **Смысл**: Вероятность того, что СВ отклонится от своего среднего больше чем на `ε`, ограничена сверху и тем меньше, чем меньше дисперсия. Это фундаментальная связь между разбросом (дисперсией) и вероятностью отклонения.

*   **Теорема Чебышёва (Закон больших чисел)**:
    Пусть `ξ₁, ξ₂, ..., ξₙ` — попарно некоррелированные СВ, дисперсии которых равномерно ограничены (D[ξᵢ] ≤ C). Тогда их среднее арифметическое сходится по вероятности к среднему их мат. ожиданий:
    `(1/n) Σξᵢ  →  (1/n) ΣM[ξᵢ]` по вероятности.
    **Частный случай (i.i.d.)**: Если СВ независимы и одинаково распределены с M[ξᵢ]=μ, то **выборочное среднее `X̄ = (1/n)Σξᵢ` сходится по вероятности к истинному среднему `μ`**.

*   **Теорема Бернулли**:
    Пусть `k` — число успехов в `n` испытаниях Бернулли с вероятностью успеха `p`. Тогда относительная частота успехов `k/n` сходится по вероятности к `p`.
    **`k/n → p`** по вероятности.
    **Смысл**: Это теоретическое обоснование статистического определения вероятности. С увеличением числа опытов частота события приближается к его вероятности.

---

### **20. Сходимость по распределению. Центральная предельная теорема (ЦПТ).**

*   **Сходимость по распределению**: Последовательность СВ `{ξₙ}` сходится по распределению к СВ `ξ`, если их функции распределения сходятся:
    `lim_{n→∞} F_n(x) = F(x)` во всех точках `x`, где F(x) непрерывна.
    **Интуиция**: Форма распределения `ξₙ` становится всё больше похожей на форму распределения `ξ`. Это более слабая форма сходимости.

#### **Центральная предельная теорема (ЦПТ)**

Это группа теорем, утверждающих, что **сумма большого числа слабо зависимых случайных величин имеет распределение, близкое к нормальному**.

*   **Ключевая идея**: Неважно, какое исходное распределение было у слагаемых (при некоторых мягких ограничениях), их сумма (или среднее) будет стремиться к нормальному распределению. Это объясняет, почему нормальное распределение так часто встречается в природе и статистике.

*   **Формулировка (Ляпунова, для i.i.d. случая)**:
    Пусть `ξ₁, ξ₂, ..., ξₙ` — независимые, одинаково распределенные СВ с конечными M[ξᵢ]=μ и D[ξᵢ]=σ².
    Рассмотрим их сумму `Sₙ = Σξᵢ`.
    Тогда **стандартизованная сумма**
    **`Zₙ = (Sₙ - M[Sₙ]) / √D[Sₙ] = (Sₙ - nμ) / (σ√n)`**
    сходится **по распределению** к стандартному нормальному распределению N(0, 1).
    `F_{Z_n}(x) → Φ(x)` (где Φ(x) - функция распределения N(0,1)).

*   **Теорема Муавра-Лапласа**:
    Исторически первая ЦПТ, является частным случаем для суммы СВ, имеющих распределение Бернулли. Она теоретически обосновывает интегральную и локальную формулы Муавра-Лапласа для аппроксимации биномиального распределения.

---

### **21. Статистическое исследование. Совокупность. Закономерность. Этапы.**

*   **Математическая статистика**: Раздел математики, посвященный методам сбора, систематизации, обработки и интерпретации статистических данных для научных и практических выводов.

*   **Статистическая совокупность**: Множество однородных по каким-либо признакам объектов или явлений, подлежащих изучению.
    *   **Генеральная совокупность**: *Все* объекты изучаемой группы.
    *   **Выборочная совокупность (выборка)**: Часть объектов, отобранных из генеральной совокупности для изучения.

*   **Статистическая закономерность**: Закономерность, проявляющаяся не в каждом отдельном явлении, а в их **массе**, в среднем, при большом числе наблюдений. (Например, закон больших чисел).

*   **Этапы статистического исследования**:
    1.  **Сбор данных (статистическое наблюдение)**: Сбор первичной информации об объекте исследования (например, путем опроса, эксперимента, из отчетности).
    2.  **Сводка и группировка данных**: Систематизация и упорядочивание собранных данных, их классификация по существенным признакам. Результат — статистические таблицы и ряды.
    3.  **Статистический анализ**: Вычисление обобщающих показателей (средние, дисперсия), оценка параметров, проверка гипотез, выявление зависимостей.
    4.  **Интерпретация результатов и выводы**: Формулировка выводов, имеющих практическое или научное значение.

---

### **22. Выборочный метод. Способы отбора. Ошибки выборки.**

*   **Выборочный метод**: Основной метод статистического исследования, при котором выводы о всей генеральной совокупности делаются на основе изучения ее части — выборки.
*   **Репрезентативность выборки**: Главное требование. Выборка должна правильно отражать структуру и свойства генеральной совокупности. Достигается случайностью отбора.

#### **Способы формирования выборки**

*   **Повторный отбор**: Каждый отобранный объект перед выбором следующего возвращается в генеральную совокупность.
*   **Бесповторный отбор**: Отобранный объект не возвращается.

#### **Виды случайного отбора**

1.  **Простой случайный отбор**: Каждый элемент имеет равные шансы попасть в выборку (как лотерея).
2.  **Механический (систематический) отбор**: Элементы отбираются через равный интервал из упорядоченного списка.
3.  **Типический (стратифицированный) отбор**: Генеральная совокупность разбивается на однородные группы (страты), и из каждой страты производится случайный отбор.
4.  **Серийный (гнездовой) отбор**: Отбираются не отдельные элементы, а целые группы (серии), которые затем подвергаются сплошному обследованию.

#### **Ошибки выборки**

*   **Ошибка репрезентативности**: Расхождение между характеристиками выборки и генеральной совокупности.
*   **Средняя ошибка выборки (μ)**: Среднеквадратическое отклонение выборочной характеристики (например, среднего) от ее генерального значения. Показывает, насколько в среднем выборочные средние отклоняются от генерального среднего.
    *   Для выборочного среднего `X̄`: `μ_X̄ = σ/√n` (повторный отбор), `μ_X̄ = σ/√n · √(1 - n/N)` (бесповторный), где `σ` - ген. СКО, `n` - объем выборки, `N` - объем ген. совокупности.
*   **Предельная ошибка выборки (Δ)**: Максимально возможная ошибка для заданной доверительной вероятности `γ`.
    *   **Δ = t · μ**, где `t` — квантиль распределения (Стьюдента или нормального), зависящий от `γ`.
    *   **Доверительный интервал**: Генеральное среднее `μ` с вероятностью `γ` лежит в интервале `(X̄ - Δ, X̄ + Δ)`.

---

### **23. Сводка и группировка. Статистические ряды.**

*   **Сводка**: Научно организованная обработка материалов наблюдения, включающая систематизацию, группировку и подсчет итогов.
*   **Группировка**: Разделение совокупности на группы по существенным для них признакам.
    *   **Типологическая**: Выделение качественно разнородных типов (напр., предприятия по форме собственности).
    *   **Структурная**: Характеризует состав однородной совокупности (напр., население по возрасту).
    *   **Аналитическая**: Выявляет взаимосвязи между признаками (напр., группировка рабочих по стажу и производительности труда).

*   **Статистический ряд**: Упорядоченное распределение единиц совокупности по какому-либо признаку.
    *   **Атрибутивный ряд**: Построен по качественному признаку (пол, профессия).
    *   **Вариационный ряд**: Построен по количественному признаку (рост, зарплата).

*   **Виды вариационных рядов**:
    *   **Дискретный**: Значения признака (варианты `xᵢ`) — целые числа. Состоит из вариант и их частот `nᵢ` (или частостей `wᵢ = nᵢ/n`).
    *   **Интервальный**: Значения признака сгруппированы в интервалы. Состоит из интервалов и частот попадания в них.

*   **Построение интервального вариационного ряда**:
    1.  Определить размах вариации `R = x_max - x_min`.
    2.  Выбрать число интервалов `k` (часто по формуле Стерджесса `k ≈ 1 + 3.322 lg(n)`).
    3.  Определить ширину интервала `h = R / k`.
    4.  Определить границы интервалов.
    5.  Подсчитать частоты `nᵢ` — число наблюдений, попавших в каждый интервал.
    6.  Графическое представление: **гистограмма** (столбчатая диаграмма, где высота столбца пропорциональна частоте/плотности), **полигон** (ломаная, соединяющая середины интервалов), **кумулята** (график накопленных частот, эмпирическая функция распределения).

---

### **24. Точечные оценки числовых характеристик. Свойства оценок. Неравенство Рао-Крамера-Фреше.**

*   **Оценка параметра**: Статистика (т.е. функция от выборки), используемая для приближенного определения неизвестного параметра `θ` генеральной совокупности. Обозначается `θ̂`.
*   **Точечная оценка**: Оценка, которая определяется одним числом.
    *   *Примеры*: Выборочное среднее `X̄` как оценка ген. среднего `μ`. Выборочная дисперсия `s²` как оценка ген. дисперсии `σ²`.

#### **Свойства точечных оценок**

Хорошая оценка должна обладать рядом желательных свойств.

1.  **Несмещенность**: Оценка `θ̂` называется **несмещенной**, если её математическое ожидание равно истинному значению параметра: `M[θ̂] = θ`.
    *   **Смещение (bias)**: `b(θ) = M[θ̂] - θ`. У несмещенной оценки смещение равно нулю.
    *   **Примеры**:
        *   `X̄ = (1/n)ΣXᵢ` — **несмещенная** оценка M[X] = μ.
        *   Выборочная дисперсия `s²_ biased = (1/n)Σ(Xᵢ - X̄)²` — **смещенная** оценка `σ²`. `M[s²_biased] = ((n-1)/n)σ²`.
        *   **Исправленная выборочная дисперсия** `s² = (1/(n-1))Σ(Xᵢ - X̄)²` — **несмещенная** оценка `σ²`.

2.  **Состоятельность**: Оценка `θ̂` называется **состоятельной**, если при увеличении объема выборки `n → ∞` она сходится по вероятности к оцениваемому параметру `θ`.
    `lim_{n→∞} P(|θ̂ₙ - θ| < ε) = 1` для любого `ε > 0`.
    *   **Интуиция**: С большой выборкой мы получаем сколь угодно точную оценку.
    *   **Достаточное условие**: Если оценка несмещенная и ее дисперсия `D[θ̂ₙ] → 0` при `n → ∞`, то она состоятельна.
    *   **Примеры**: `X̄` и `s²` являются состоятельными оценками `μ` и `σ²`.

3.  **Эффективность**: Несмещенная оценка `θ̂` называется **эффективной**, если она имеет наименьшую возможную дисперсию среди всех несмещенных оценок данного параметра `θ`.
    *   **Интуиция**: Эффективная оценка дает наименьший разброс, т.е. является самой "точной".
    *   **Относительная эффективность**: Для двух несмещенных оценок `θ̂₁` и `θ̂₂`, их относительная эффективность — это `Eff(θ̂₁, θ̂₂) = D[θ̂₂] / D[θ̂₁]`.

#### **Неравенство Рао-Крамера-Фреше**

*   **Назначение**: Дает **нижнюю границу** для дисперсии любой несмещенной оценки. Помогает определить, является ли оценка эффективной.
*   **Информация Фишера**: Величина, показывающая, сколько информации о параметре `θ` содержится в одном наблюдении.
    `I(θ) = M[(∂ln(f(X, θ)) / ∂θ)²] = -M[∂²ln(f(X, θ)) / ∂θ²]`, где `f(X, θ)` — плотность распределения.
*   **Неравенство**: Для любой несмещенной оценки `θ̂` параметра `θ` на основе выборки объема `n` выполняется:
    **`D[θ̂] ≥ 1 / (n · I(θ))`**
*   **Вывод**: Если дисперсия некоторой несмещенной оценки `θ̂` совпадает с этой нижней границей (`D[θ̂] = 1 / (nI(θ))`), то эта оценка является **эффективной**.

---

### **25. Интервальные оценки. Доверительные интервалы для параметров нормального распределения.**

*   **Проблема точечной оценки**: Она почти никогда не совпадает с истинным значением параметра.
*   **Интервальная оценка**: Указывает интервал, который с заданной высокой вероятностью накрывает истинное значение параметра.
*   **Доверительный интервал**: Случайный интервал `(θ̂₁, θ̂₂)` (границы зависят от выборки), который с заданной **доверительной вероятностью γ** (или уровнем надежности, обычно 0.9, 0.95, 0.99) содержит неизвестный параметр `θ`.
    `P(θ̂₁ < θ < θ̂₂) = γ`.
*   **Уровень значимости**: `α = 1 - γ`. Вероятность того, что интервал не накроет параметр.

#### **Доверительные интервалы для параметров нормального распределения N(μ, σ²)**

1.  **Для мат. ожидания μ при известной дисперсии σ²**:
    *   Используется статистика `Z = (X̄ - μ) / (σ/√n) ~ N(0, 1)`.
    *   Интервал: **`X̄ ± z_(1-α/2) · (σ/√n)`**
        где `z_(1-α/2)` — квантиль стандартного нормального распределения (например, для γ=0.95, α=0.05, `z_0.975` ≈ 1.96).

2.  **Для мат. ожидания μ при неизвестной дисперсии σ²**:
    *   Используется статистика `T = (X̄ - μ) / (s/√n) ~ t(n-1)` (распределение Стьюдента с `n-1` степенями свободы), где `s` — исправленное СКО.
    *   Интервал: **`X̄ ± t_(1-α/2, n-1) · (s/√n)`**
        где `t_(1-α/2, n-1)` — квантиль распределения Стьюдента.

3.  **Для дисперсии σ² (и СКО σ) при известном μ**:
    *   Используется статистика `χ² = Σ(Xᵢ - μ)² / σ² ~ χ²(n)`.
    *   Интервал для σ²: **`(Σ(Xᵢ - μ)² / χ²_(1-α/2, n), Σ(Xᵢ - μ)² / χ²_(α/2, n))`**
        где `χ²` — квантили распределения хи-квадрат с `n` степенями свободы.

4.  **Для дисперсии σ² (и СКО σ) при неизвестном μ**:
    *   Используется статистика `χ² = (n-1)s² / σ² ~ χ²(n-1)`.
    *   Интервал для σ²: **`((n-1)s² / χ²_(1-α/2, n-1), (n-1)s² / χ²_(α/2, n-1))`**

---

### **26. Статистические гипотезы и их виды. Проверка гипотез.**

*   **Статистическая гипотеза**: Предположение о параметрах или виде распределения генеральной совокупности, которое можно проверить на основе выборки.

#### **Виды гипотез**

*   **Нулевая гипотеза (H₀)**: Основная, проверяемая гипотеза. Обычно это гипотеза об отсутствии различий, эффекта, связи (`μ₁ = μ₂`, `p = 0.5`, `ρ = 0`). Мы исходим из того, что она верна, пока не доказано обратное.
*   **Альтернативная гипотеза (H₁ или Hₐ)**: Гипотеза, которая принимается, если H₀ отвергается. Конкурирует с нулевой.
    *   **Двусторонняя**: `H₁: θ ≠ θ₀`
    *   **Правосторонняя**: `H₁: θ > θ₀`
    *   **Левосторонняя**: `H₁: θ < θ₀`

#### **Проверка статистических гипотез**

1.  **Выбор статистического критерия (статистики)**: Функция от выборки `T(X₁,...,Xₙ)`, распределение которой известно (по крайней мере, при условии верности H₀).
2.  **Задание уровня значимости α**: Вероятность совершить ошибку I рода (обычно 0.05, 0.01, 0.1).
3.  **Определение критической области (W)**: Множество значений критерия, при попадании в которое нулевая гипотеза отвергается. Границы этой области (критические точки) определяются уровнем значимости `α` и видом `H₁`.
    *   **Область принятия гипотезы**: Дополнение к W.
4.  **Расчет наблюдаемого значения критерия (T_набл)** по данным выборки.
5.  **Принятие решения**:
    *   Если `T_набл` ∈ W (попадает в критическую область), то **H₀ отвергается** в пользу H₁. Результат статистически значим.
    *   Если `T_набл` ∉ W (не попадает в критическую область), то **нет оснований отвергать H₀**. Это не значит, что H₀ доказана, а лишь то, что данные не противоречат ей.

#### **Ошибки при проверке гипотез**

| Решение / Истина | H₀ верна       | H₀ неверна     |
|------------------|----------------|----------------|
| **Принять H₀**   | Верное решение | **Ошибка II рода** (β) |
| **Отклонить H₀** | **Ошибка I рода** (α) | Верное решение (Мощность 1-β) |

*   **Ошибка I рода (α)**: Отвергнуть верную H₀. Вероятность `α` — это уровень значимости.
*   **Ошибка II рода (β)**: Принять неверную H₀.
*   **Мощность критерия (1-β)**: Вероятность отвергнуть неверную H₀. Чем выше мощность, тем лучше критерий.

---

### **27. Проверка гипотез о типе закона распределения. Критерии согласия.**

*   **Задача**: Проверить гипотезу H₀ о том, что выборка извлечена из генеральной совокупности с определенным законом распределения (например, нормальным, Пуассона).
*   **Критерий согласия**: Статистический критерий для проверки такой гипотезы. Он измеряет "расхождение" между эмпирическим (наблюдаемым) распределением и теоретическим (предполагаемым).

#### **Критерий χ² (хи-квадрат) Пирсона**

*   **Применение**: Для проверки простых и сложных гипотез о виде распределения. Особенно удобен для дискретных распределений или сгруппированных данных.
*   **Алгоритм**:
    1.  Разбить весь диапазон значений на `k` интервалов (или взять `k` дискретных значений).
    2.  Подсчитать **эмпирические частоты** `nᵢ` — число наблюдений, попавших в i-й интервал.
    3.  Вычислить **теоретические вероятности** `pᵢ` попадания в i-й интервал, исходя из гипотезы H₀. Если параметры распределения (μ, σ²) неизвестны, их оценивают по выборке.
    4.  Вычислить **теоретические частоты** `Eᵢ = n · pᵢ`.
    5.  Рассчитать **наблюдаемое значение критерия**:
        **`χ²_набл = Σ_{i=1 to k} (nᵢ - Eᵢ)² / Eᵢ`**
    6.  Определить **критическое значение** `χ²_крит(α, df)` по таблице распределения хи-квадрат, где `α` — уровень значимости, `df` — число степеней свободы.
        `df = k - 1 - m`, где `m` — число параметров, оцененных по выборке.
    7.  **Решение**: Если `χ²_набл > χ²_крит`, то H₀ отвергается.

#### **Критерий Колмогорова**

*   **Применение**: Для проверки простых гипотез (когда вид и **все параметры** распределения полностью заданы). Не требует группировки данных.
*   **Алгоритм**:
    1.  Построить **эмпирическую функцию распределения** `F*(x)` (ступенчатая функция, показывающая долю наблюдений ≤ x).
    2.  Построить **теоретическую функцию распределения** `F(x)` согласно гипотезе H₀.
    3.  Найти **максимальное абсолютное расхождение** между ними:
        **`D_n = sup_x |F*(x) - F(x)|`**
    4.  Рассчитать статистику `λ = D_n · √n`.
    5.  Сравнить `λ` с критическим значением `λ_крит(α)` из таблиц распределения Колмогорова.
    6.  **Решение**: Если `λ > λ_крит`, то H₀ отвергается.

---

### **28. Гипотезы об однородности двух выборок. ANOVA.**

*   **Задача**: Определить, можно ли считать две (или более) выборки извлеченными из одной и той же генеральной совокупности или из совокупностей с одинаковыми параметрами.

*   **Формулировки гипотез (для двух выборок)**:
    *   О равенстве средних: `H₀: μ₁ = μ₂`.
    *   О равенстве дисперсий: `H₀: σ₁² = σ₂²`.
    *   О совпадении законов распределения в целом.

*   **Критерии проверки**:
    *   **Для средних**:
        *   **t-критерий Стьюдента** для независимых выборок (если дисперсии равны).
        *   **Критерий Уэлча** (модификация t-критерия, если дисперсии не равны).
        *   **U-критерий Манна-Уитни** (непараметрический аналог, если распределения не нормальные).
    *   **Для дисперсий**:
        *   **F-критерий Фишера**: `F_набл = s₁²/s₂²` (где `s₁² > s₂²`). Сравнивается с F-критическим.

#### **Однофакторный дисперсионный анализ (ANOVA)**

*   **Назначение**: Проверка гипотезы о равенстве средних **трёх и более** совокупностей (`H₀: μ₁ = μ₂ = ... = μₖ`).
*   **Идея**: Сравнить изменчивость **между группами** с изменчивостью **внутри групп**. Если изменчивость между группами значительно больше, чем внутри, то средние, вероятно, различаются.
*   **Модель**: `Xᵢⱼ = μ + αᵢ + εᵢⱼ`, где `Xᵢⱼ` — j-е наблюдение в i-й группе, `μ` — общее среднее, `αᵢ` — эффект i-й группы, `εᵢⱼ` — случайная ошибка.
*   **Алгоритм**:
    1.  Общая вариация `SS_total` разлагается на две компоненты:
        `SS_total = SS_between + SS_within`
        *   `SS_between` (межгрупповая сумма квадратов) — изменчивость, обусловленная различием групповых средних.
        *   `SS_within` (внутригрупповая сумма квадратов) — изменчивость, обусловленная случайностью внутри каждой группы.
    2.  Вычисляются средние квадраты (Mean Squares): `MS = SS / df`.
        *   `MS_between = SS_between / (k-1)`
        *   `MS_within = SS_within / (n-k)`
    3.  Рассчитывается **F-статистика**:
        **`F = MS_between / MS_within`**
    4.  Полученное значение F сравнивается с критическим значением F-распределения с `k-1` и `n-k` степенями свободы. Если `F_набл > F_крит`, гипотеза о равенстве средних отвергается.

---

### **29. Парная линейная регрессия. Выборочный коэффициент корреляции.**

*   **Задача**: Изучить линейную зависимость между двумя количественными переменными X (независимая, предиктор) и Y (зависимая, отклик).
*   **Модель**: `yᵢ = β₀ + β₁xᵢ + εᵢ`, где `β₀` и `β₁` — неизвестные параметры (коэффициенты регрессии), `εᵢ` — случайные ошибки.

#### **Построение прямой регрессии**

*   **Метод наименьших квадратов (МНК)**: Параметры `b₀` и `b₁` (оценки для `β₀` и `β₁`) подбираются так, чтобы минимизировать сумму квадратов остатков (ошибок):
    `Σ(yᵢ - ŷᵢ)² = Σ(yᵢ - (b₀ + b₁xᵢ))² → min`
*   **Формулы для коэффициентов**:
    *   **`b₁ = Cov(X, Y) / D(X) = (Σ(xᵢ-x̄)(yᵢ-ȳ)) / (Σ(xᵢ-x̄)²) `**
    *   **`b₀ = ȳ - b₁x̄`**

#### **Выборочный коэффициент корреляции (Пирсона)**

*   **Определение**: Оценка коэффициента корреляции `ρ` по выборке. Обозначается `r`.
    **`r = Cov(X, Y) / (s_x · s_y) = (Σ(xᵢ-x̄)(yᵢ-ȳ)) / (√(Σ(xᵢ-x̄)²) · √(Σ(yᵢ-ȳ)²))`**
*   **Свойства**: Аналогичны свойствам `ρ`. ` -1 ≤ r ≤ 1`. Показывает силу и направление **линейной** связи в выборке.

#### **Проверка гипотезы о значимости коэффициента корреляции**

*   **Задача**: Проверить, является ли наблюдаемый `r` статистически значимо отличным от нуля.
*   **Гипотезы**: `H₀: ρ = 0` (линейной связи нет), `H₁: ρ ≠ 0` (линейная связь есть).
*   **Критерий**: Используется t-статистика:
    **`t_набл = r · √(n-2) / √(1-r²)`**
*   **Решение**: Полученное значение `t_набл` сравнивается с критическим значением `t_крит(α, n-2)` из распределения Стьюдента. Если `|t_набл| > t_крит`, то `H₀` отвергается, и корреляция считается статистически значимой.

---

### **30. Парная регрессия. Корреляционное отношение. Нелинейная регрессия.**

*   **Ограничение `r`**: Коэффициент корреляции Пирсона измеряет только **линейную** связь. Если связь есть, но она нелинейная (например, параболическая), `r` может быть близок к нулю.

#### **Корреляционное отношение (η)**

*   **Назначение**: Измеряет силу **любой** (не обязательно линейной) корреляционной зависимости.
*   **Идея**: Основано на идеях дисперсионного анализа. Сравнивает межгрупповую дисперсию (объясненную регрессией) с общей дисперсией.
*   **Расчет**:
    1.  Данные по `x` группируются.
    2.  Вычисляются межгрупповая (`SS_between`) и общая (`SS_total`) суммы квадратов для переменной `y`.
    3.  **Индекс корреляции**: `R² = SS_between / SS_total`. Показывает, какую долю общей вариации `y` объясняет регрессия `y` на `x`. `0 ≤ R² ≤ 1`.
    4.  **Корреляционное отношение**: `η = √R²`.

*   **Свойства**:
    *   `0 ≤ η ≤ 1`.
    *   `η = 0` <=> нет корреляционной зависимости.
    *   `η = 1` <=> есть точная функциональная зависимость.
    *   **Важное свойство**: `η² ≥ r²`. Равенство достигается только при строгой линейной зависимости. Разница `η² - r²` характеризует степень нелинейности связи.

#### **Нелинейная регрессия**

*   **Задача**: Аппроксимировать зависимость `y` от `x` нелинейной функцией.
*   **Примеры моделей**:
    *   **Полиномиальная**: `y = β₀ + β₁x + β₂x² + ...`
    *   **Степенная**: `y = β₀x^{β₁}`
    *   **Показательная**: `y = β₀e^{β₁x}`
    *   **Логарифмическая**: `y = β₀ + β₁ln(x)`

*   **Метод решения**: Многие нелинейные модели можно **линеаризовать** путем замены переменных и затем применить обычный МНК.
    *   *Пример (степенная)*: `ln(y) = ln(β₀) + β₁ln(x)`. Замена `Y = ln(y)`, `X = ln(x)`, `B₀ = ln(β₀)`, `B₁ = β₁` приводит к линейной модели `Y = B₀ + B₁X`.
*   Если линеаризация невозможна, используются численные итерационные методы для минимизации суммы квадратов остатков.